{"paragraphs":[{"text":"%md\n\n# Spark Machine Learning\n\n1. Spark.mllib vs Spark.ml\n2. SparkML concepts\n3. Feature Selection\n4. Introduction to some of the libraries useful for Text Analysis","user":"anonymous","dateUpdated":"2018-03-06T09:14:29-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Spark Machine Learning</h1>\n<ol>\n  <li>Spark.mllib vs Spark.ml</li>\n  <li>SparkML concepts</li>\n  <li>Feature Selection</li>\n  <li>Introduction to some of the libraries useful for Text Analysis</li>\n</ol>\n</div>"}]},"apps":[],"jobName":"paragraph_1520317765079_-650209452","id":"20180306-012925_62964199","dateCreated":"2018-03-06T01:29:25-0500","dateStarted":"2018-03-06T09:14:29-0500","dateFinished":"2018-03-06T09:14:30-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:13631"},{"text":"%md\n\n### DataFrame\n\nThis ML API uses DataFrame from Spark SQL as an ML dataset, which can hold a variety of data types. \nE.g., a DataFrame could have different columns storing text, feature vectors, true labels, and predictions.\n\n### Transformer\n\nA Transformer is an algorithm which can transform one DataFrame into another DataFrame. \nE.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions.\n\nTechnically, a Transformer implements a method transform(), which converts one DataFrame into another, \ngenerally by appending one or more columns. For example:\n\n* A feature transformer might take a DataFrame, read a column (e.g., text), map it into a new column (e.g., feature vectors), and output a new DataFrame with the mapped column appended.\n* A learning model might take a DataFrame, read the column containing feature vectors, predict the label for each feature vector, and output a new DataFrame with predicted labels appended as a column.\n\n\n### Estimator\n\nAn Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. \nE.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model.\nAn Estimator abstracts the concept of a learning algorithm or any algorithm that fits or trains on data. \n\nTechnically, an Estimator implements a method fit(), which accepts a DataFrame and produces a Model, which is a Transformer. \nFor example, a learning algorithm such as LogisticRegression is an Estimator, and calling fit() trains a LogisticRegressionModel, which is a Model and hence a Transformer.\n\n### Pipeline\n\n\nA Pipeline chains multiple Transformers and Estimators together to specify an ML workflow.\nIn machine learning, it is common to run a sequence of algorithms to process and learn from data. \nE.g., a simple text document processing workflow might include several stages:\n\n* Split each document’s text into words.\n* Convert each document’s words into a numerical feature vector.\n* Learn a prediction model using the feature vectors and labels.\n\n#### Training\n\n<center> <img src='https://spark.apache.org/docs/latest/img/ml-Pipeline.png' width=800> </center>\n\n<hr> \n\n#### Test\n\n<center> <img src='https://spark.apache.org/docs/latest/img/ml-PipelineModel.png' width=\"800\">  </center> \n\n\n### Parameter\n\nAll Transformers and Estimators now share a common API for specifying parameters.\n","user":"anonymous","dateUpdated":"2018-03-06T01:32:27-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>DataFrame</h3>\n<p>This ML API uses DataFrame from Spark SQL as an ML dataset, which can hold a variety of data types.<br/>E.g., a DataFrame could have different columns storing text, feature vectors, true labels, and predictions.</p>\n<h3>Transformer</h3>\n<p>A Transformer is an algorithm which can transform one DataFrame into another DataFrame.<br/>E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions.</p>\n<p>Technically, a Transformer implements a method transform(), which converts one DataFrame into another,<br/>generally by appending one or more columns. For example:</p>\n<ul>\n  <li>A feature transformer might take a DataFrame, read a column (e.g., text), map it into a new column (e.g., feature vectors), and output a new DataFrame with the mapped column appended.</li>\n  <li>A learning model might take a DataFrame, read the column containing feature vectors, predict the label for each feature vector, and output a new DataFrame with predicted labels appended as a column.</li>\n</ul>\n<h3>Estimator</h3>\n<p>An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer.<br/>E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model.<br/>An Estimator abstracts the concept of a learning algorithm or any algorithm that fits or trains on data. </p>\n<p>Technically, an Estimator implements a method fit(), which accepts a DataFrame and produces a Model, which is a Transformer.<br/>For example, a learning algorithm such as LogisticRegression is an Estimator, and calling fit() trains a LogisticRegressionModel, which is a Model and hence a Transformer.</p>\n<h3>Pipeline</h3>\n<p>A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow.<br/>In machine learning, it is common to run a sequence of algorithms to process and learn from data.<br/>E.g., a simple text document processing workflow might include several stages:</p>\n<ul>\n  <li>Split each document’s text into words.</li>\n  <li>Convert each document’s words into a numerical feature vector.</li>\n  <li>Learn a prediction model using the feature vectors and labels.</li>\n</ul>\n<h4>Training</h4>\n<center> <img src='https://spark.apache.org/docs/latest/img/ml-Pipeline.png' width=800> </center>\n<hr>\n<h4>Test</h4>\n<center> <img src='https://spark.apache.org/docs/latest/img/ml-PipelineModel.png' width=\"800\">  </center>\n<h3>Parameter</h3>\n<p>All Transformers and Estimators now share a common API for specifying parameters.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520300671701_1746460590","id":"20180305-204431_1769458411","dateCreated":"2018-03-05T20:44:31-0500","dateStarted":"2018-03-06T01:31:55-0500","dateFinished":"2018-03-06T01:31:55-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13632"},{"text":"import org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.linalg.{Vector, Vectors}\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.sql.Row\n","user":"anonymous","dateUpdated":"2018-03-06T09:18:24-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.linalg.{Vector, Vectors}\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.sql.Row\n"}]},"apps":[],"jobName":"paragraph_1520305502252_-2084360273","id":"20180305-220502_686215489","dateCreated":"2018-03-05T22:05:02-0500","dateStarted":"2018-03-06T09:18:25-0500","dateFinished":"2018-03-06T09:18:26-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13633"},{"text":"// Prepare training data from a list of (label, features) tuples.\n\nval training = spark.createDataFrame(Seq(\n  (1.0, Vectors.dense(0.0, 1.1, 0.1)),\n  (0.0, Vectors.dense(2.0, 1.0, -1.0)),\n  (0.0, Vectors.dense(2.0, 1.3, 1.0)),\n  (1.0, Vectors.dense(0.0, 1.2, -0.5))\n)).toDF(\"label\", \"features\")\n","user":"anonymous","dateUpdated":"2018-03-06T01:32:27-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"training: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"}]},"apps":[],"jobName":"paragraph_1520305732366_-227625023","id":"20180305-220852_369464448","dateCreated":"2018-03-05T22:08:52-0500","dateStarted":"2018-03-06T01:31:55-0500","dateFinished":"2018-03-06T01:31:58-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13634"},{"text":"training.show()","user":"anonymous","dateUpdated":"2018-03-06T01:32:27-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----+--------------+\n|label|      features|\n+-----+--------------+\n|  1.0| [0.0,1.1,0.1]|\n|  0.0|[2.0,1.0,-1.0]|\n|  0.0| [2.0,1.3,1.0]|\n|  1.0|[0.0,1.2,-0.5]|\n+-----+--------------+\n\n"}]},"apps":[],"jobName":"paragraph_1520305731934_-49101533","id":"20180305-220851_1990502508","dateCreated":"2018-03-05T22:08:51-0500","dateStarted":"2018-03-06T01:31:58-0500","dateFinished":"2018-03-06T01:31:58-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13635"},{"text":"\n// Create a LogisticRegression instance. This instance is an Estimator.\nval lr = new LogisticRegression()\n// Print out the parameters, documentation, and any default values.\nprintln(s\"LogisticRegression parameters:\\n ${lr.explainParams()}\\n\")\n","user":"anonymous","dateUpdated":"2018-03-06T01:32:27-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"lr: org.apache.spark.ml.classification.LogisticRegression = logreg_5060f2271178\nLogisticRegression parameters:\n aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\nfamily: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial. (default: auto)\nfeaturesCol: features column name (default: features)\nfitIntercept: whether to fit an intercept term (default: true)\nlabelCol: label column name (default: label)\nmaxIter: maximum number of iterations (>= 0) (default: 100)\npredictionCol: prediction column name (default: prediction)\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\nregParam: regularization parameter (>= 0) (default: 0.0)\nstandardization: whether to standardize the training features before fitting the model (default: true)\nthreshold: threshold in binary classification prediction, in range [0, 1] (default: 0.5)\nthresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)\ntol: the convergence tolerance for iterative algorithms (>= 0) (default: 1.0E-6)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\n\n"}]},"apps":[],"jobName":"paragraph_1520305731795_-94117154","id":"20180305-220851_1244215667","dateCreated":"2018-03-05T22:08:51-0500","dateStarted":"2018-03-06T01:31:58-0500","dateFinished":"2018-03-06T01:31:58-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13636"},{"text":"\n// We may set parameters using setter methods.\nlr.setMaxIter(10)\n  .setRegParam(0.01)\n","user":"anonymous","dateUpdated":"2018-03-06T01:32:27-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res266: lr.type = logreg_5060f2271178\n"}]},"apps":[],"jobName":"paragraph_1520305731605_-121049577","id":"20180305-220851_1630732620","dateCreated":"2018-03-05T22:08:51-0500","dateStarted":"2018-03-06T01:31:58-0500","dateFinished":"2018-03-06T01:31:59-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13637"},{"text":"// Learn a LogisticRegression model. This uses the parameters stored in lr.\nval model1 = lr.fit(training)\n// Since model1 is a Model (i.e., a Transformer produced by an Estimator),\n// we can view the parameters it used during fit().\n// This prints the parameter (name: value) pairs, where names are unique IDs for this\n// LogisticRegression instance.\nprintln(s\"Model 1 was fit using parameters: ${model1.parent.extractParamMap}\")","user":"anonymous","dateUpdated":"2018-03-06T01:32:27-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"model1: org.apache.spark.ml.classification.LogisticRegressionModel = logreg_5060f2271178\nModel 1 was fit using parameters: {\n\tlogreg_5060f2271178-aggregationDepth: 2,\n\tlogreg_5060f2271178-elasticNetParam: 0.0,\n\tlogreg_5060f2271178-family: auto,\n\tlogreg_5060f2271178-featuresCol: features,\n\tlogreg_5060f2271178-fitIntercept: true,\n\tlogreg_5060f2271178-labelCol: label,\n\tlogreg_5060f2271178-maxIter: 10,\n\tlogreg_5060f2271178-predictionCol: prediction,\n\tlogreg_5060f2271178-probabilityCol: probability,\n\tlogreg_5060f2271178-rawPredictionCol: rawPrediction,\n\tlogreg_5060f2271178-regParam: 0.01,\n\tlogreg_5060f2271178-standardization: true,\n\tlogreg_5060f2271178-threshold: 0.5,\n\tlogreg_5060f2271178-tol: 1.0E-6\n}\n"}]},"apps":[],"jobName":"paragraph_1520305731071_-1194499008","id":"20180305-220851_376198990","dateCreated":"2018-03-05T22:08:51-0500","dateStarted":"2018-03-06T01:31:59-0500","dateFinished":"2018-03-06T01:32:01-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13638"},{"text":"\n// We may alternatively specify parameters using a ParamMap,\n// which supports several methods for specifying parameters.\nval paramMap = ParamMap(lr.maxIter -> 20)\n  .put(lr.maxIter, 30)  // Specify 1 Param. This overwrites the original maxIter.\n  .put(lr.regParam -> 0.1, lr.threshold -> 0.55)  // Specify multiple Params.\n","user":"anonymous","dateUpdated":"2018-03-06T01:32:27-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"paramMap: org.apache.spark.ml.param.ParamMap =\n{\n\tlogreg_5060f2271178-maxIter: 30,\n\tlogreg_5060f2271178-regParam: 0.1,\n\tlogreg_5060f2271178-threshold: 0.55\n}\n"}]},"apps":[],"jobName":"paragraph_1520310603579_838129954","id":"20180305-233003_860778172","dateCreated":"2018-03-05T23:30:03-0500","dateStarted":"2018-03-06T01:31:59-0500","dateFinished":"2018-03-06T01:32:01-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13639"},{"text":"// One can also combine ParamMaps.\nval paramMap2 = ParamMap(lr.probabilityCol -> \"myProbability\")  // Change output column name.\nval paramMapCombined = paramMap ++ paramMap2","user":"anonymous","dateUpdated":"2018-03-06T01:32:27-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"paramMap2: org.apache.spark.ml.param.ParamMap =\n{\n\tlogreg_5060f2271178-probabilityCol: myProbability\n}\nparamMapCombined: org.apache.spark.ml.param.ParamMap =\n{\n\tlogreg_5060f2271178-maxIter: 30,\n\tlogreg_5060f2271178-probabilityCol: myProbability,\n\tlogreg_5060f2271178-regParam: 0.1,\n\tlogreg_5060f2271178-threshold: 0.55\n}\n"}]},"apps":[],"jobName":"paragraph_1520310603442_792344835","id":"20180305-233003_745027388","dateCreated":"2018-03-05T23:30:03-0500","dateStarted":"2018-03-06T01:32:01-0500","dateFinished":"2018-03-06T01:32:01-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13640"},{"text":"// Now learn a new model using the paramMapCombined parameters.\n// paramMapCombined overrides all parameters set earlier via lr.set* methods.\nval model2 = lr.fit(training, paramMapCombined)\nprintln(s\"Model 2 was fit using parameters: ${model2.parent.extractParamMap}\")","user":"anonymous","dateUpdated":"2018-03-06T01:32:27-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"model2: org.apache.spark.ml.classification.LogisticRegressionModel = logreg_5060f2271178\nModel 2 was fit using parameters: {\n\tlogreg_5060f2271178-aggregationDepth: 2,\n\tlogreg_5060f2271178-elasticNetParam: 0.0,\n\tlogreg_5060f2271178-family: auto,\n\tlogreg_5060f2271178-featuresCol: features,\n\tlogreg_5060f2271178-fitIntercept: true,\n\tlogreg_5060f2271178-labelCol: label,\n\tlogreg_5060f2271178-maxIter: 30,\n\tlogreg_5060f2271178-predictionCol: prediction,\n\tlogreg_5060f2271178-probabilityCol: myProbability,\n\tlogreg_5060f2271178-rawPredictionCol: rawPrediction,\n\tlogreg_5060f2271178-regParam: 0.1,\n\tlogreg_5060f2271178-standardization: true,\n\tlogreg_5060f2271178-threshold: 0.55,\n\tlogreg_5060f2271178-tol: 1.0E-6\n}\n"}]},"apps":[],"jobName":"paragraph_1520310603111_1005880475","id":"20180305-233003_994724780","dateCreated":"2018-03-05T23:30:03-0500","dateStarted":"2018-03-06T01:32:02-0500","dateFinished":"2018-03-06T01:32:03-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13641"},{"text":"// Prepare test data.\nval test = spark.createDataFrame(Seq(\n  (1.0, Vectors.dense(-1.0, 1.5, 1.3)),\n  (0.0, Vectors.dense(3.0, 2.0, -0.1)),\n  (1.0, Vectors.dense(0.0, 2.2, -1.5))\n)).toDF(\"label\", \"features\")\n","user":"anonymous","dateUpdated":"2018-03-06T01:32:27-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"test: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"}]},"apps":[],"jobName":"paragraph_1520310602988_953169875","id":"20180305-233002_1740490667","dateCreated":"2018-03-05T23:30:02-0500","dateStarted":"2018-03-06T01:32:02-0500","dateFinished":"2018-03-06T01:32:03-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13642"},{"text":"test.show","user":"anonymous","dateUpdated":"2018-03-06T01:32:27-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----+--------------+\n|label|      features|\n+-----+--------------+\n|  1.0|[-1.0,1.5,1.3]|\n|  0.0|[3.0,2.0,-0.1]|\n|  1.0|[0.0,2.2,-1.5]|\n+-----+--------------+\n\n"}]},"apps":[],"jobName":"paragraph_1520312086869_340735517","id":"20180305-235446_1855239477","dateCreated":"2018-03-05T23:54:46-0500","dateStarted":"2018-03-06T01:32:03-0500","dateFinished":"2018-03-06T01:32:03-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13643"},{"text":"// Make predictions on test data using the Transformer.transform() method.\n// LogisticRegression.transform will only use the 'features' column.\n// Note that model2.transform() outputs a 'myProbability' column instead of the usual\n// 'probability' column since we renamed the lr.probabilityCol parameter previously.\nmodel2.transform(test)\n  .select(\"features\", \"label\", \"myProbability\", \"prediction\")\n  .collect()\n  .foreach { case Row(features: Vector, label: Double, prob: Vector, prediction: Double) =>\n    println(s\"($features, $label) -> prob=$prob, prediction=$prediction\")\n  }","user":"anonymous","dateUpdated":"2018-03-06T01:32:27-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"([-1.0,1.5,1.3], 1.0) -> prob=[0.05707304171034101,0.9429269582896589], prediction=1.0\n([3.0,2.0,-0.1], 0.0) -> prob=[0.9238522311704136,0.07614776882958646], prediction=0.0\n([0.0,2.2,-1.5], 1.0) -> prob=[0.10972776114780064,0.8902722388521994], prediction=1.0\n"}]},"apps":[],"jobName":"paragraph_1520310700317_196089665","id":"20180305-233140_2113592632","dateCreated":"2018-03-05T23:31:40-0500","dateStarted":"2018-03-06T01:32:03-0500","dateFinished":"2018-03-06T01:32:04-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13644"},{"title":"Creating a Pipeline","text":"import org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.feature.{HashingTF, Tokenizer}\nimport org.apache.spark.ml.linalg.Vector\nimport org.apache.spark.sql.Row\n\n// Prepare training documents from a list of (id, text, label) tuples.\nval training = spark.createDataFrame(Seq(\n  (0L, \"a b c d e spark\", 1.0),\n  (1L, \"b d\", 0.0),\n  (2L, \"spark f g h\", 1.0),\n  (3L, \"hadoop mapreduce\", 0.0)\n)).toDF(\"id\", \"text\", \"label\")\n\n// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\nval tokenizer = new Tokenizer()\n  .setInputCol(\"text\")\n  .setOutputCol(\"words\")\n\nval hashingTF = new HashingTF()\n  .setNumFeatures(1000)\n  .setInputCol(tokenizer.getOutputCol)\n  .setOutputCol(\"features\")\nval lr = new LogisticRegression()\n  .setMaxIter(10)\n  .setRegParam(0.001)\nval pipeline = new Pipeline()\n  .setStages(Array(tokenizer, hashingTF, lr))\n\n// Fit the pipeline to training documents.\nval model = pipeline.fit(training)\n\n// Now we can optionally save the fitted pipeline to disk\nmodel.write.overwrite().save(\"/tmp/spark-logistic-regression-model\")\n\n// We can also save this unfit pipeline to disk\npipeline.write.overwrite().save(\"/tmp/unfit-lr-model\")\n\n// And load it back in during production\nval sameModel = PipelineModel.load(\"/tmp/spark-logistic-regression-model\")\n","user":"anonymous","dateUpdated":"2018-03-06T01:32:27-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false,"title":true,"lineNumbers":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.feature.{HashingTF, Tokenizer}\nimport org.apache.spark.ml.linalg.Vector\nimport org.apache.spark.sql.Row\ntraining: org.apache.spark.sql.DataFrame = [id: bigint, text: string ... 1 more field]\ntokenizer: org.apache.spark.ml.feature.Tokenizer = tok_1f0aa0b26d90\nhashingTF: org.apache.spark.ml.feature.HashingTF = hashingTF_422b79274a82\nlr: org.apache.spark.ml.classification.LogisticRegression = logreg_bfe52c1b1bd3\npipeline: org.apache.spark.ml.Pipeline = pipeline_58d02ab5b9b2\nmodel: org.apache.spark.ml.PipelineModel = pipeline_58d02ab5b9b2\nsameModel: org.apache.spark.ml.PipelineModel = pipeline_58d02ab5b9b2\n"}]},"apps":[],"jobName":"paragraph_1520310699959_-1142067010","id":"20180305-233139_663618570","dateCreated":"2018-03-05T23:31:39-0500","dateStarted":"2018-03-06T01:32:04-0500","dateFinished":"2018-03-06T01:32:07-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13645"},{"text":"// Prepare test documents, which are unlabeled (id, text) tuples.\nval test = spark.createDataFrame(Seq(\n  (4L, \"spark i j k\"),\n  (5L, \"l m n\"),\n  (6L, \"spark hadoop spark\"),\n  (7L, \"apache hadoop\")\n)).toDF(\"id\", \"text\")\n\n// Make predictions on test documents.\nmodel.transform(test)\n  .select(\"id\", \"text\", \"probability\", \"prediction\")\n  .collect()\n  .foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) =>\n    println(s\"($id, $text) --> prob=$prob, prediction=$prediction\")\n  }","user":"anonymous","dateUpdated":"2018-03-06T01:32:27-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"test: org.apache.spark.sql.DataFrame = [id: bigint, text: string]\n(4, spark i j k) --> prob=[0.15964077387874118,0.8403592261212589], prediction=1.0\n(5, l m n) --> prob=[0.8378325685476612,0.16216743145233875], prediction=0.0\n(6, spark hadoop spark) --> prob=[0.06926633132976273,0.9307336686702373], prediction=1.0\n(7, apache hadoop) --> prob=[0.9821575333444208,0.01784246665557917], prediction=0.0\n"}]},"apps":[],"jobName":"paragraph_1520314388609_511057210","id":"20180306-003308_467529411","dateCreated":"2018-03-06T00:33:08-0500","dateStarted":"2018-03-06T01:32:04-0500","dateFinished":"2018-03-06T01:32:08-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13646"},{"text":"","user":"anonymous","dateUpdated":"2018-03-06T01:32:27-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520314321166_179544865","id":"20180306-003201_161206539","dateCreated":"2018-03-06T00:32:01-0500","dateStarted":"2018-03-06T00:32:05-0500","dateFinished":"2018-03-06T00:32:06-0500","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13647"},{"text":"%md\n\n## Feature Selection\n\n* Extraction: Extracting features from “raw” data\n* Transformation: Scaling, converting, or modifying features\n* Selection: Selecting a subset from a larger set of features\n<!--* Locality Sensitive Hashing (LSH): This class of algorithms combines aspects of feature transformation with other algorithms.-->\n\n\n**tf–idf**, **TFIDF** or **term frequency–inverse document frequency** is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.","user":"anonymous","dateUpdated":"2018-03-08T16:28:55-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Feature Selection</h2>\n<ul>\n  <li>Extraction: Extracting features from “raw” data</li>\n  <li>Transformation: Scaling, converting, or modifying features</li>\n  <li>Selection: Selecting a subset from a larger set of features<br/><!--* Locality Sensitive Hashing (LSH): This class of algorithms combines aspects of feature transformation with other algorithms.--></li>\n</ul>\n<p><strong>tf–idf</strong>, <strong>TFIDF</strong> or <strong>term frequency–inverse document frequency</strong> is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520312484221_1003472337","id":"20180306-000124_1418879341","dateCreated":"2018-03-06T00:01:24-0500","dateStarted":"2018-03-08T16:28:55-0500","dateFinished":"2018-03-08T16:28:56-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13648"},{"text":"%md\n\n$$ IDF(t,D)= \\log \\frac{|D|+1}{DF(t,D)+1} $$\n\nWe denote a term by \\\\( t \\\\), a document by  \\\\( d  \\\\), and the corpus by  \\\\( D  \\\\). Term frequency  \\\\( TF(t,d)  \\\\) is the number of times that term  \\\\( t  \\\\) appears in document  \\\\( d  \\\\), while document frequency  \\\\( DF(t,D)  \\\\) is the number of documents that contains term \\\\( t  \\\\).\n\nAnd \n\n$$ TFIDF(t,d,D)=TF(t,d)⋅IDF(t,D) $$","user":"anonymous","dateUpdated":"2018-03-06T01:33:07-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>$$ IDF(t,D)= \\log \\frac{|D|+1}{DF(t,D)+1} $$</p>\n<p>We denote a term by \\( t \\), a document by \\( d \\), and the corpus by \\( D \\). Term frequency \\( TF(t,d) \\) is the number of times that term \\( t \\) appears in document \\( d \\), while document frequency \\( DF(t,D) \\) is the number of documents that contains term \\( t \\).</p>\n<p>And </p>\n<p>$$ TFIDF(t,d,D)=TF(t,d)⋅IDF(t,D) $$</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520310575099_-2133731847","id":"20180305-232935_982479688","dateCreated":"2018-03-05T23:29:35-0500","dateStarted":"2018-03-06T01:33:07-0500","dateFinished":"2018-03-06T01:33:07-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13649"},{"text":"import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}\n\nval sentenceData = spark.createDataFrame(Seq(\n  (0.0, \"Hi I heard about Spark\"),\n  (0.0, \"I wish Java could use case classes\"),\n  (1.0, \"Logistic regression models are neat\")\n)).toDF(\"label\", \"sentence\")\n\nval tokenizer = new Tokenizer().setInputCol(\"sentence\").setOutputCol(\"words\")\nval wordsData = tokenizer.transform(sentenceData)\n\nval hashingTF = new HashingTF()\n  .setInputCol(\"words\").setOutputCol(\"rawFeatures\").setNumFeatures(20)\n\nval featurizedData = hashingTF.transform(wordsData)\n\n\n// alternatively, CountVectorizer can also be used to get term frequency vectors\n\nval idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\")\nval idfModel = idf.fit(featurizedData)\n\nval rescaledData = idfModel.transform(featurizedData)\nrescaledData.select(\"label\", \"features\").show()","user":"anonymous","dateUpdated":"2018-03-06T01:32:27-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false,"lineNumbers":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}\nsentenceData: org.apache.spark.sql.DataFrame = [label: double, sentence: string]\ntokenizer: org.apache.spark.ml.feature.Tokenizer = tok_e882d432b453\nwordsData: org.apache.spark.sql.DataFrame = [label: double, sentence: string ... 1 more field]\nhashingTF: org.apache.spark.ml.feature.HashingTF = hashingTF_67ae2859daaa\nfeaturizedData: org.apache.spark.sql.DataFrame = [label: double, sentence: string ... 2 more fields]\nidf: org.apache.spark.ml.feature.IDF = idf_cb5260bfc907\nidfModel: org.apache.spark.ml.feature.IDFModel = idf_cb5260bfc907\nrescaledData: org.apache.spark.sql.DataFrame = [label: double, sentence: string ... 3 more fields]\n+-----+--------------------+\n|label|            features|\n+-----+--------------------+\n|  0.0|(20,[0,5,9,17],[0...|\n|  0.0|(20,[2,7,9,13,15]...|\n|  1.0|(20,[4,6,13,15,18...|\n+-----+--------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1520310574602_2044656533","id":"20180305-232934_255213386","dateCreated":"2018-03-05T23:29:34-0500","dateStarted":"2018-03-06T01:32:07-0500","dateFinished":"2018-03-06T01:32:10-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13650"},{"text":"%md\n\n## Tokenizer","user":"anonymous","dateUpdated":"2018-03-06T01:33:14-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Tokenizer</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1520316060407_-672038556","id":"20180306-010100_1451390206","dateCreated":"2018-03-06T01:01:00-0500","dateStarted":"2018-03-06T01:33:14-0500","dateFinished":"2018-03-06T01:33:14-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13651"},{"text":"\nimport org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions._\n\nval sentenceDataFrame = spark.createDataFrame(Seq(\n  (0, \"Hi I heard about Spark\"),\n  (1, \"I wish Java could use case classes\"),\n  (2, \"Logistic,regression,models,are,neat\")\n)).toDF(\"id\", \"sentence\")\n\nval tokenizer = new Tokenizer().setInputCol(\"sentence\").setOutputCol(\"words\")\nval regexTokenizer = new RegexTokenizer()\n  .setInputCol(\"sentence\")\n  .setOutputCol(\"words\")\n  .setPattern(\"\\\\W\") // alternatively .setPattern(\"\\\\w+\").setGaps(false)\n\nval countTokens = udf { (words: Seq[String]) => words.length }\n\nval tokenized = tokenizer.transform(sentenceDataFrame)\ntokenized.select(\"sentence\", \"words\")\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(false)\n\nval regexTokenized = regexTokenizer.transform(sentenceDataFrame)\nregexTokenized.select(\"sentence\", \"words\")\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(false)","user":"anonymous","dateUpdated":"2018-03-06T01:32:28-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false,"lineNumbers":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions._\nsentenceDataFrame: org.apache.spark.sql.DataFrame = [id: int, sentence: string]\ntokenizer: org.apache.spark.ml.feature.Tokenizer = tok_7f5de28b5f9e\nregexTokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_adcb2fff0937\ncountTokens: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,IntegerType,Some(List(ArrayType(StringType,true))))\ntokenized: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 1 more field]\n+-----------------------------------+------------------------------------------+------+\n|sentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\n|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n+-----------------------------------+------------------------------------------+------+\n\nregexTokenized: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 1 more field]\n+-----------------------------------+------------------------------------------+------+\n|sentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\n|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n+-----------------------------------+------------------------------------------+------+\n\n"}]},"apps":[],"jobName":"paragraph_1520316047193_-1395213246","id":"20180306-010047_259476799","dateCreated":"2018-03-06T01:00:47-0500","dateStarted":"2018-03-06T01:32:08-0500","dateFinished":"2018-03-06T01:32:14-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13652"},{"text":"%md\n\n## Stop Words","user":"anonymous","dateUpdated":"2018-03-06T01:33:17-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Stop Words</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1520316200711_-808457494","id":"20180306-010320_36890792","dateCreated":"2018-03-06T01:03:20-0500","dateStarted":"2018-03-06T01:33:17-0500","dateFinished":"2018-03-06T01:33:17-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13653"},{"text":"import org.apache.spark.ml.feature.StopWordsRemover\n\nval remover = new StopWordsRemover()\n  .setInputCol(\"raw\")\n  .setOutputCol(\"filtered\")\n\nval dataSet = spark.createDataFrame(Seq(\n  (0, Seq(\"I\", \"saw\", \"the\", \"red\", \"balloon\")),\n  (1, Seq(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n)).toDF(\"id\", \"raw\")\n\nremover.transform(dataSet).show(false)","user":"anonymous","dateUpdated":"2018-03-06T01:32:28-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.feature.StopWordsRemover\nremover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_5317a1625f1e\ndataSet: org.apache.spark.sql.DataFrame = [id: int, raw: array<string>]\n+---+----------------------------+--------------------+\n|id |raw                         |filtered            |\n+---+----------------------------+--------------------+\n|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n+---+----------------------------+--------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1520316047050_-1449462841","id":"20180306-010047_1942583935","dateCreated":"2018-03-06T01:00:47-0500","dateStarted":"2018-03-06T01:32:10-0500","dateFinished":"2018-03-06T01:32:17-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13654"},{"text":"%md\n\n## word2vec\n\n**Word2Vec** is an Estimator which takes sequences of words representing documents and trains a **Word2VecModel**. The model maps each word to a unique fixed-size vector. The Word2VecModel transforms each document into a vector using the average of all words in the document; this vector can then be used as features for prediction, document similarity calculations, etc.","user":"anonymous","dateUpdated":"2018-03-06T01:33:27-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>word2vec</h2>\n<p><strong>Word2Vec</strong> is an Estimator which takes sequences of words representing documents and trains a <strong>Word2VecModel</strong>. The model maps each word to a unique fixed-size vector. The Word2VecModel transforms each document into a vector using the average of all words in the document; this vector can then be used as features for prediction, document similarity calculations, etc.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520310574411_-1979832287","id":"20180305-232934_2054880421","dateCreated":"2018-03-05T23:29:34-0500","dateStarted":"2018-03-06T01:33:27-0500","dateFinished":"2018-03-06T01:33:27-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13655"},{"text":"import org.apache.spark.ml.feature.Word2Vec\nimport org.apache.spark.ml.linalg.Vector\nimport org.apache.spark.sql.Row\n\n// Input data: Each row is a bag of words from a sentence or document.\nval documentDF = spark.createDataFrame(Seq(\n  \"Hi I heard about Spark\".split(\" \"),\n  \"I wish Java could use case classes\".split(\" \"),\n  \"Logistic regression models are neat\".split(\" \")\n).map(Tuple1.apply)).toDF(\"text\")\n\n// Learn a mapping from words to Vectors.\nval word2Vec = new Word2Vec()\n  .setInputCol(\"text\")\n  .setOutputCol(\"result\")\n  .setVectorSize(3)\n  .setMinCount(0)\nval model = word2Vec.fit(documentDF)\n\nval result = model.transform(documentDF)\nresult.collect().foreach { case Row(text: Seq[_], features: Vector) =>\n  println(s\"Text: [${text.mkString(\", \")}] => \\nVector: $features\\n\") }","user":"anonymous","dateUpdated":"2018-03-06T01:32:27-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false,"lineNumbers":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.feature.Word2Vec\nimport org.apache.spark.ml.linalg.Vector\nimport org.apache.spark.sql.Row\ndocumentDF: org.apache.spark.sql.DataFrame = [text: array<string>]\nword2Vec: org.apache.spark.ml.feature.Word2Vec = w2v_f70dba711416\nmodel: org.apache.spark.ml.feature.Word2VecModel = w2v_f70dba711416\nresult: org.apache.spark.sql.DataFrame = [text: array<string>, result: vector]\nText: [Hi, I, heard, about, Spark] => \nVector: [-0.008142343163490296,0.02051363289356232,0.03255096450448036]\n\nText: [I, wish, Java, could, use, case, classes] => \nVector: [0.043090314205203734,0.035048123182994974,0.023512658663094044]\n\nText: [Logistic, regression, models, are, neat] => \nVector: [0.038572299480438235,-0.03250147425569594,-0.01552378609776497]\n\n"}]},"apps":[],"jobName":"paragraph_1520310574072_-1740133722","id":"20180305-232934_1798648649","dateCreated":"2018-03-05T23:29:34-0500","dateStarted":"2018-03-06T00:39:00-0500","dateFinished":"2018-03-06T00:39:07-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13656"},{"text":"%md\n\n\n## CountVectorizer\n\n id | texts\n----|----------\n 0  | Array(\"a\", \"b\", \"c\")\n 1  | Array(\"a\", \"b\", \"b\", \"c\", \"a\")\n \n <hr>\n \n  id | texts                           | vector\n----|---------------------------------|---------------\n 0  | Array(\"a\", \"b\", \"c\")            | (3,[0,1,2],[1.0,1.0,1.0])\n 1  | Array(\"a\", \"b\", \"b\", \"c\", \"a\")  | (3,[0,1,2],[2.0,2.0,1.0])\n \n ","user":"anonymous","dateUpdated":"2018-03-06T01:33:43-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>CountVectorizer</h2>\n<table>\n  <thead>\n    <tr>\n      <th>id </th>\n      <th>texts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0 </td>\n      <td>Array(&ldquo;a&rdquo;, &ldquo;b&rdquo;, &ldquo;c&rdquo;)</td>\n    </tr>\n    <tr>\n      <td>1 </td>\n      <td>Array(&ldquo;a&rdquo;, &ldquo;b&rdquo;, &ldquo;b&rdquo;, &ldquo;c&rdquo;, &ldquo;a&rdquo;)</td>\n    </tr>\n  </tbody>\n</table>\n<p><hr></p>\n<table>\n  <thead>\n    <tr>\n      <th>id </th>\n      <th>texts </th>\n      <th>vector</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0 </td>\n      <td>Array(&ldquo;a&rdquo;, &ldquo;b&rdquo;, &ldquo;c&rdquo;) </td>\n      <td>(3,[0,1,2],[1.0,1.0,1.0])</td>\n    </tr>\n    <tr>\n      <td>1 </td>\n      <td>Array(&ldquo;a&rdquo;, &ldquo;b&rdquo;, &ldquo;b&rdquo;, &ldquo;c&rdquo;, &ldquo;a&rdquo;) </td>\n      <td>(3,[0,1,2],[2.0,2.0,1.0])</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}]},"apps":[],"jobName":"paragraph_1520310573876_-1763218656","id":"20180305-232933_999576360","dateCreated":"2018-03-05T23:29:33-0500","dateStarted":"2018-03-06T01:33:43-0500","dateFinished":"2018-03-06T01:33:43-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13657"},{"text":"import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n\nval df = spark.createDataFrame(Seq(\n  (0, Array(\"a\", \"b\", \"c\")),\n  (1, Array(\"a\", \"b\", \"b\", \"c\", \"a\"))\n)).toDF(\"id\", \"words\")\n\n// fit a CountVectorizerModel from the corpus\nval cvModel: CountVectorizerModel = new CountVectorizer()\n  .setInputCol(\"words\")\n  .setOutputCol(\"features\")\n  .setVocabSize(3)\n  .setMinDF(2)\n  .fit(df)\n\n// alternatively, define CountVectorizerModel with a-priori vocabulary\nval cvm = new CountVectorizerModel(Array(\"a\", \"b\", \"c\"))\n  .setInputCol(\"words\")\n  .setOutputCol(\"features\")\n\ncvModel.transform(df).show(false)","user":"anonymous","dateUpdated":"2018-03-06T01:32:27-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false,"lineNumbers":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\ndf: org.apache.spark.sql.DataFrame = [id: int, words: array<string>]\ncvModel: org.apache.spark.ml.feature.CountVectorizerModel = cntVec_1a946eae4536\ncvm: org.apache.spark.ml.feature.CountVectorizerModel = cntVecModel_f4c824ec4371\n+---+---------------+-------------------------+\n|id |words          |features                 |\n+---+---------------+-------------------------+\n|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+---+---------------+-------------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1520310573522_-1526982831","id":"20180305-232933_1129878048","dateCreated":"2018-03-05T23:29:33-0500","dateStarted":"2018-03-06T00:44:45-0500","dateFinished":"2018-03-06T00:44:49-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13658"}],"name":"Intro to Spark ML","id":"2DAJ4YAT7","angularObjects":{"2D68M8N2H:shared_process":[],"2D62B8DA4:shared_process":[],"2D95K6JXV:shared_process":[],"2D9SVDGDY:shared_process":[],"2D8ABD2T1:shared_process":[],"2D94CH8DX:shared_process":[],"2D6486X8P:shared_process":[],"2D5XKJMVA:shared_process":[],"2D9QG3Q4G:shared_process":[],"2D7A8YNDT:shared_process":[],"2D88X52CN:shared_process":[],"2D9G13CGT:shared_process":[],"2D887VDK2:shared_process":[],"2D5W34ZCW:shared_process":[],"2D6J9VCTY:shared_process":[],"2D7415XZ8:shared_process":[],"2D7K3C2D3:shared_process":[],"2D7UGPWAA:shared_process":[],"2D913KKJ5:shared_process":[],"2D65SWMUP:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}